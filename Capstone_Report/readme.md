# How many stars will I give? Predicting ratings of Amazon reviews

	A Capstone project: 
          	Tonia Chu
	Under the mentorship: 
   		Srdjan Santic (Consulting Data Scientist at the Research Center for Cheminformatics)
	For the course: 
 		Data Science Career Track (Springboard)

## I. INTRODUCTION
Nowadays, a massive amount of reviews is available online. Besides offering a valuable source of information, these informational contents generated by users, strongly impact the purchase decision of customers. Many consumers are effectively influenced by online reviews when making their purchase decisions. Relying on online reviews has thus become a second nature for consumers.

In their research process, consumers want to find useful information as quickly as possible. However, searching and comparing text reviews can be frustrating for users. Indeed, the massive amount of text reviews as well as its unstructured text format prevent the user from choosing a product with ease. The star-rating, i.e. stars from 1 to 5 on Amazon, rather than its text content gives a quick overview of the product quality. This numerical information is the No. 1 factor used in an early phase by consumers to compare products before making their purchase decision.

However, many product reviews (from other platforms than Amazon) are not accompanied by a scale rating system, consisting only of a textual evaluation. In this case, it becomes daunting and time-consuming to compare different products in order to eventually make a choice between them. Therefore, models able to predict the user rating from the text review are critically important. Getting an overall sense of a textual review could in turn improve consumer experience. Also, it can help business to increase sales, and improve the product by understanding customers' needs and pain-points.

The purpose of this project is to develop models that are able to predict the user rating from the text review. While our model is built to work with any kind of product, the review dataset provided by Amazon only includes Clothing and Shoes reviews.
 
## II. Deeper dive into the data set
We get dataset from Amazon product data, which contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 ~ July 2014. The dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).

In this project, we use 5-core dataset of Clothing and Shoes, which is subset of the data in which all users and items have at least 5 reviews.  

Sample review is as following:

    "reviewerID": "A2SUAM1J3GNN3B",  
	"asin": "0000013714",  
	"reviewerName": "J. McDonald",  
	"helpful": [2, 3],  
	"reviewText": "I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.  The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!",  
	"overall": 5.0,  
	"summary": "Heavenly Highway Hymns",  
	"unixReviewTime": 1252800000,  
	"reviewTime": "09 13, 2009"  

### 1. Preparing Amazon dataset
First we load dataset from json file and rename column 'overall' to 'Rating'. 
```python
review_df = pd.read_json('Amazon_reviews/Clothing_Shoes_and_Jewelry_5.json', orient='records', lines=True)

# change column name 
review_df = review_df.rename(columns={'overall': 'Rating'})
```
There are 278677 records in the Clothing and Shoes dataset. 

### 2. Preliminary Analysis
* __Summary of the dataset:__
```
Number of reviews:  278677  

Number of unique reviewers:  39387  
Prop of unique reviewers:  0.141  

Number of unique products:  23033  
Prop of unique products:  0.083  

Average rating score:  4.245  
```
* __Distribution of rating score__

![rating-fr](https://github.com/anxin16/Capstone-Project-3/blob/master/Figures/rating-fr.png)

* __Distribution of rating propotion__

![rating-pro](https://github.com/anxin16/Capstone-Project-3/blob/master/Figures/rating-pro1.png)

* __Subset data__

Because text analysis is very time cosumming, we just use subset of the dataset to go through our process of text normalization, feature engineering and machine learning. When all are set, we can use whole dataset or other bigger dataset to get better model.

So we select reviews before 2012 as our sub dataset. There are 16434 records in it. 

Distribution of the ratings is as following:
```
Rating
1      690
2      861
3     1518
4     3363
5    10002
```

## III. Pre-processing —— Text Normalization
For natural language processing (NLP) and text analytics, pre-processing is very important. To carry out different operations and analyze text, we need to process and parse textual data into more easy-to-interpret formats. All machine learning (ML) algorithms usually work with input features that are numeric in nature. To get to that, we need to clean, normalize, and pre-process the initial textual data. 

Usually text corpora and other textual data in their native raw format are not well formatted and standardized. And text data is highly unstructured! Text pre-processing, involves using a variety of techniques to convert raw text into well-defined sequences of linguistic components that have standard structure and notation.

Text normalization is defined as a process that consists of a series of steps that should be followed to wrangle, clean, and standardize textual data into a form that could be consumed by other NLP and analytics systems and applications as input. Besides tokenization, various other techniques include cleaning text, case conversion, correcting spellings, removing stopwords and other unnecessary terms, stemming, and lemmatization. Text normalization is also often called text cleansing or wrangling.

Below are various techniques used in the process of text normalization:

* Cleaning Text
* Tokenizing Text
* Removing Special Characters
* Expanding Contractions
* Case Conversions
* Removing Stopwords
* Correcting Words
* Stemming
* Lemmatization

We will use most of the techniques in this project.

### 1. Expanding Contractions
Contractions are shortened version of words or syllables. They exist in either written or spoken forms. Shortened versions of existing words are created by removing specific letters and sounds. In case of English contractions, they are often created by removing one of the vowels from the word.

By nature, contractions do pose a problem for NLP and text analytics because, to start with, we have a special apostrophe character in the word. Ideally, we can have a proper mapping for contractions and their corresponding expansions and then use it to expand all the contractions in our text. 
```python
from contractions import CONTRACTION_MAP

# Define function to expand contractions
def expand_contractions(text):
    contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())),flags=re.IGNORECASE|re.DOTALL)
    def expand_match(contraction):
        match = contraction.group(0)
        first_char = match[0]
        expanded_contraction = CONTRACTION_MAP.get(match)\
                        if CONTRACTION_MAP.get(match)\
                        else CONTRACTION_MAP.get(match.lower())
        expanded_contraction = first_char+expanded_contraction[1:]
        return expanded_contraction
    
    expanded_text = contractions_pattern.sub(expand_match, text)
    expanded_text = re.sub("'", "", expanded_text)
    return expanded_text
```

### 2. Removing Special Characters
One important task in text normalization involves removing unnecessary and special characters. These may be special symbols or even punctuation that occurs in sentences. This step is often performed before or after tokenization. The main reason for doing so is because often punctuation or special characters do not have much significance when we analyze the text and utilize it for extracting features or information based on NLP and ML.
```python
# Define the function to remove special characters
def remove_characters(text):
    text = text.strip()
    PATTERN = '[^a-zA-Z0-9 ]' # only extract alpha-numeric characters
    filtered_text = re.sub(PATTERN, '', text)
    return filtered_text
```

### 3. Tokenizing Text
Tokenization can be defined as the process of breaking down or splitting textual data into smaller meaningful components called tokens.

**Sentence tokenization** is the process of splitting a text corpus into sentences that act as the first level of tokens which the corpus is comprised of. This is also known as sentence segmentation , because we try to segment the text into meaningful sentences.

**Word tokenization** is the process of splitting or segmenting sentences into their constituent words. A sentence is a collection of words, and with tokenization we essentially split a sentence into a list of words that can be used to reconstruct the sentence.
```python
# Define the tokenization function
def tokenize_text(text):
    word_tokens = nltk.word_tokenize(text)
    tokens = [token.strip() for token in word_tokens]
    return tokens
```

### 4. Removing Stopwords
Stopwords are words that have little or no significance. They are usually removed from text during processing so as to retain words having maximum significance and context. Stopwords are usually words that end up occurring the most if you aggregated any corpus of text based on singular tokens and checked their frequencies. Words like a, the , me , and so on are stopwords.
```python
from nltk.corpus import stopwords
# In Python, searching a set is much faster than searching a list, 
# so convert the stop words to a set
stopword_list = set(stopwords.words("english"))

# Define function to remove stopwords
def remove_stopwords(tokens):
    filtered_tokens = [token for token in tokens if token not in stopword_list]
    return filtered_tokens
```

### 5. Correcting Words
One of the main challenges faced in text normalization is the presence of incorrect words in the text. The definition of incorrect here covers words that have spelling mistakes as well as words with several letters repeated that do not contribute much to its overall significance.

**5.1 Correcting Repeating Characters**
```python
from nltk.corpus import wordnet

# Define function to remove repeated characters
def remove_repeated_characters(tokens):
    repeat_pattern = re.compile(r'(\w*)(\w)\2(\w*)')
    match_substitution = r'\1\2\3'
    def replace(old_word):
        if wordnet.synsets(old_word):
            return old_word
        new_word = repeat_pattern.sub(match_substitution, old_word)
        return replace(new_word) if new_word != old_word else new_word

    correct_tokens = [replace(word) for word in tokens]
    return correct_tokens
```

**5.2 Correcting Spellings**
```python
from collections import Counter

# Generate a map of frequently occurring words in English and their counts
"""
The input corpus we use is a file containing several books from the Gutenberg corpus and also 
a list of most frequent words from Wiktionary and the British National Corpus. You can find 
the file under the name big.txt or download it from http://norvig.com/big.txt and use it.
"""
def tokens(text):
    """
    Get all words from the corpus
    """
    return re.findall('[a-z]+', text.lower())

WORDS = tokens(open('big.txt').read())
WORD_COUNTS = Counter(WORDS)
```
```python
# Define functions that compute sets of words that are one and two edits away from input word.
def edits1(word):
    "All edits that are one edit away from `word`."
    letters    = 'abcdefghijklmnopqrstuvwxyz'
    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]
    deletes    = [L + R[1:]               for L, R in splits if R]
    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]
    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]
    inserts    = [L + c + R               for L, R in splits for c in letters]
    return set(deletes + transposes + replaces + inserts)

def edits2(word): 
    "All edits that are two edits away from `word`."
    return (e2 for e1 in edits1(word) for e2 in edits1(e1))
```
```python
# Define function that returns a subset of words from our candidate set of words obtained from 
# the edit functions, based on whether they occur in our vocabulary dictionary WORD_COUNTS.
# This gives us a list of valid words from our set of candidate words.
def known(words): 
    "The subset of `words` that appear in the dictionary of WORD_COUNTS."
    return set(w for w in words if w in WORD_COUNTS)
```
```python
# Define function to correct words
def correct(words):
    # Get the best correct spellings for the input words
    def candidates(word): 
        # Generate possible spelling corrections for word.
        # Priority is for edit distance 0, then 1, then 2, else defaults to the input word itself.
        candidates = known([word]) or known(edits1(word)) or known(edits2(word)) or [word]
        return candidates
    
    corrected_words = [max(candidates(word), key=WORD_COUNTS.get) for word in words]
    return corrected_words
```

### 6. Lemmatization
The process of lemmatization is to remove word affixes to get to a base form of the word. The base form is also known as the root word, or the lemma, will always be present in the dictionary.
```python
import spacy
nlp = spacy.load("en")

# Define function for Lemmatization
def Lemmatize_tokens(tokens):
    doc = ' '.join(tokens)
    Lemmatized_tokens = [token.lemma_ for token in nlp(doc)]
    return Lemmatized_tokens
```

### 7. Text Normalization
```python
def normalize_corpus(corpus):
    normalized_corpus = []    
    for text in corpus:
        text = text.lower()
        text = expand_contractions(text)
        text = remove_characters(text)
        tokens = tokenize_text(text)
        tokens = remove_stopwords(tokens)
        tokens = remove_repeated_characters(tokens)
        tokens = correct(tokens)
        tokens = Lemmatize_tokens(tokens)
        text = ' '.join(tokens)
        normalized_corpus.append(text)
                    
    return normalized_corpus
```

After above steps, we get normalized texts of Amazon reviews. 

## IV. Feature Engineering —— Feature Extraction 
**Feature engineering** is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. 

In ML terminology, features are unique, measurable attributes or properties for each observation or data point in a dataset. Features are usually numeric in nature and can be absolute numeric values or categorical features that can be encoded as binary features for each category in the list using a process called one-hot encoding. The process of extracting and selecting features is both art and science, and this process is called *feature extraction* or *feature engineering*.

The *Vector Space Model* is a concept and model that is very useful in case we are dealing with textual data and is very popular in information retrieval and document ranking. The Vector Space Model, also known as the *Term Vector Model*, is defined as a mathematical and algebraic model for transforming and representing text documents as numeric vectors of specific terms that form the vector dimensions.

We will be implementing the following feature-extraction techniques in this project:  
* Bag of Words model  
* TF-IDF model  
* Averaged Word Vectors  
* TF-IDF Weighted Averaged Word Vectors   

### 1. Bag of Words Model
The Bag of Words model is perhaps one of the simplest yet most powerful techniques to extract features from text documents. The essence of this model is to convert text documents into vectors such that each document is converted into a vector that
represents the frequency of all the distinct words that are present in the document vector space for that specific document. 
```python
from sklearn.feature_extraction.text import CountVectorizer

def bow_extractor(corpus, ngram_range=(1,1)):
    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range, max_features = 5000)
    features = vectorizer.fit_transform(corpus)
    return vectorizer, features
```

### 2. TF-IDF Model
TF-IDF stands for Term Frequency-Inverse Document Frequency, a combination of two metrics: *term frequency* and *inverse document frequency*.

Mathematically, TF-IDF is the product of two metrics and can be represented as $tfidf = tfxidf$, where *term frequency*(tf) and *inverse-document frequency*(idf) represent the two metrics.
```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Define function to directly compute the tfidf-based feature vectors for documents from the raw documents.
def tfidf_extractor(corpus, ngram_range=(1,1)):
    vectorizer = TfidfVectorizer(min_df=1,
                                 norm='l2',
                                 smooth_idf=True,
                                 use_idf=True,
                                 ngram_range=ngram_range,
                                 max_features = 5000)
    features = vectorizer.fit_transform(corpus)
    return vectorizer, features
```

### 3. Averaged Word Vectors
In this technique, we will use an average weighted word vectorization scheme, where for each text document we will extract all the tokens of the text document, and for each token in the document we will capture the subsequent word vector if present in the vocabulary. We will sum up all the word vectors and divide the result by the total number of words matched in the vocabulary to get a final resulting averaged word vector representation for the text document.
```python
import numpy as np    

# Define function to average word vectors for a text document
def average_word_vectors(words, model, vocabulary, num_features):
    
    feature_vector = np.zeros((num_features,),dtype="float64")
    nwords = 0.
    
    for word in words:
        if word in vocabulary: 
            nwords = nwords + 1.
            feature_vector = np.add(feature_vector, model[word])
    
    if nwords:
        feature_vector = np.divide(feature_vector, nwords)
        
    return feature_vector
```
```python
# Generalize above function for a corpus of documents  
def averaged_word_vectorizer(corpus, model, num_features):
    vocabulary = set(model.wv.index2word)
    features = [average_word_vectors(sentence, model, vocabulary, num_features) for sentence in corpus]
    return np.array(features)
```

### 4. TF-IDF Weighted Averaged Word Vectors
Our previous vectorizer simply sums up all the word vectors pertaining to any document based on the words in the model vocabulary and calculates a simple average by dividing with the count of matched words. Now we use a new and novel technique of weighing each matched word vector with the word TF-TDF score and summing up all the word vectors for a document and dividing it by the sum of all the TF-IDF weights of the matched words in the document. This would basically give us a TF-IDF weighted averaged word vector for each document.
```python
# Define function to compute tfidf weighted averaged word vector for a document
def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):
    
    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] 
                   if tfidf_vocabulary.get(word) 
                   else 0 for word in words]    
    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}
    
    feature_vector = np.zeros((num_features,),dtype="float64")
    vocabulary = set(model.wv.index2word)
    wts = 0.
    for word in words:
        if word in vocabulary: 
            word_vector = model[word]
            weighted_word_vector = word_tfidf_map[word] * word_vector
            wts = wts + word_tfidf_map[word]
            feature_vector = np.add(feature_vector, weighted_word_vector)
    if wts:
        feature_vector = np.divide(feature_vector, wts)
        
    return feature_vector
```
```python
# Generalize above function for a corpus of documents
def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, 
                                   tfidf_vocabulary, model, num_features):
                                       
    docs_tfidfs = [(doc, doc_tfidf) 
                   for doc, doc_tfidf 
                   in zip(corpus, tfidf_vectors)]
    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,
                                   model, num_features)
                    for tokenized_sentence, tfidf in docs_tfidfs]
    return np.array(features) 
```

## V. Modeling and Machine learning
In this step, we need develop models to predict ratings from reviews with machine learning. Here we will use Classification Algorithms. Classification algorithms are supervised ML algorithms that are used to classify, categorize, or label data points based on what it has observed in the past. There are mainly three processes classification algorithms go through:
* Training
* Evaluation
* Hyperparameter tuning

Before develop and evaluate models, first we split data into train and test sets, then extract all the necessary features of training and test data using the preceding feature extractors. 

### 1. Classification models
There are various types of classification algorithms, but our focus remains on text classification. So we will touch upon a couple of algorithms that are quite effective for text classification. These algorithms are the following:

**1.1 Logistic Regression**

_Logistic regression_, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.

**1.2 Multinomial Naive Bayes**

_Multinomial Naive Bayes_ implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts). This algorithm is a special case of the popular naïve Bayes algorithm, which is used specifically for prediction and classification tasks where we have more than two classes.

**1.3 Linear Support Vector Classification**

_Linear Support Vector Classification_ is similar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.

**1.4 SGD Classifier**

__Stochastic Gradient Descent (SGD)__ is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.

SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.

The advantages of Stochastic Gradient Descent are:  
* Efficiency.
* Ease of implementation (lots of opportunities for code tuning).
	
The disadvantages of Stochastic Gradient Descent include:  
* SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.
* SGD is sensitive to feature scaling.

SGD Classifier is linear classifiers with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).

**1.5 Random Forest Classifier**

A _random forest_ is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).

In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.

```python
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier

# Build ML methods
clf_LR = LogisticRegression()
clf_MNB = MultinomialNB()
clf_LSVC = LinearSVC()
clf_SGD = SGDClassifier()
clf_RFC = RandomForestClassifier()
```

### 2. Evaluating Classification Models
Training and building models are an important part of the whole analytics lifecycle, but even more important is knowing how well these models are performing. Performance of classification models is usually based on how well they predict outcomes for new data points. Usually this performance is measured against a test or holdout dataset that consists of data points which was not used to influence or train the classifier in any way. This test dataset usually has several observations and corresponding labels.

We extract features in the same way as it was followed when training the model. These features are fed to the already trained model, and we obtain predictions for each data point. These predictions are then matched with the actual labels to see how well or how accurately the model has predicted.

Several metrics determine a model’s prediction performance, but we will mainly focus on the following metrics:  
* Accuracy
* Recall

_Accuracy_ is defined as the overall accuracy or proportion of correct predictions of the model. We have our correct predictions in the numerator divided by all the outcomes in the denominator. 

_Recall_ is defined as the number of instances of the positive class that were correctly predicted. This is also known as _hit rate_, _coverage_, or _sensitivity_. We have our correct predictions for the positive class in the numerator divided by
correct and missed instances for the positive class, giving us the hit rate. This metric is good to check the hit rate of each category. 

We use the metrics module from scikit-learn, which is very powerful and helps in computing these metrics with a single function.
```python
from sklearn.metrics import accuracy_score, classification_report

def get_metrics(true_labels, predicted_labels):
    print ('Accuracy: ', accuracy_score(true_labels,predicted_labels))
    print (classification_report(true_labels, predicted_labels))
```

We defined a function that trains the model using a ML algorithm and the training data, performs predictions on the test data using the trained model, and then evaluates the predictions using the preceding function to give us the model performance:
```python
def train_predict_evaluate_model(classifier, 
                                 train_features, train_labels, 
                                 test_features, test_labels):
    # build model    
    classifier.fit(train_features, train_labels)
    # predict using model
    train_predictions = classifier.predict(train_features)
    test_predictions = classifier.predict(test_features) 
    # evaluate model prediction performance 
    print ('Training set performance:')
    get_metrics(true_labels=train_labels, predicted_labels=train_predictions)
    print ('Test set performance:')
    get_metrics(true_labels=test_labels, predicted_labels=test_predictions)
    return test_predictions    
```

Now we train, predict, and evaluate above classification models using all the different types of features. Below is the result form we get from recall values:

![Evaluate-result-form](https://github.com/anxin16/Capstone-Project-3/blob/master/Figures/Evaluate-result-form.png)

From the above form, we can get following results:  
1. Among the four kinds for features, tfidf is the best feature. It can get the highest accuracy while save time for training models. And bag of words is the second best feature. tfidf weighted averaged word vector is the worst feature because of the lowest accuracy and longest training time.   
2. Among the five cllasification models, Logistic Regression is the best one and Linear Support Vector Classification is the second best one. Random Forest Classifier is the worst one.  
3. Among the five levels ratings, rating 5 have the best predicting result while rating 1 have the worst result. Accuracy of predicting rating 5 is very high for each model. But accuracy of predicting rating 1, 2 and 3 are very low. One reason for this may be that most records have rating 5 and least records have rating 1. More training data can get better model.
4. The models marked in pink color are relatively best models. We will work on them with hyperparameter Tuning.

### 3. Confusion Matrix of Models
A _confusion matrix_ is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class. The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another). 

Followings are the confusion matrixs for the above marked models:

**3.1 Logistic Regression using Bag of words features:**  

0  |  1  |  2  |  3  |  4  |   5
--- | --- |--- | --- | --- | ---
1 | 54 | 42 |  19 |  20  |  79
2 | 20 | 62 |  60 |  47  | 119
3 | 21 | 45 | 129 | 155  | 219
4 |  8 | 16 |  93 | 318  | 773
5 |  9 | 11 |  77 | 245  | 3359

**3.2 Logistic Regression with Tfidf features:**  

0  |  1  |  2  |  3  |  4  |   5
--- | --- |--- | --- | --- | ---
1 | 32 | 28 | 27 |  21  | 106  
2 | 12 | 26 | 59 |  47  | 164  
3 |  3 | 19 | 99 | 161  | 287  
4 |  2 |  6 | 55 | 272  | 873  
5 |  0 |  4 | 21 | 157  | 3519  

**3.3 Multinomial Naive Bayes using Bag of words features:**

0  |  1  |  2  |  3  |  4  |   5
--- | --- |--- | --- | --- | ---
1 | 65 | 48 |  32 |  23  |  46
2 | 30 | 55 |  83 |  55  |  85
3 | 14 | 53 | 151 | 172  | 179
4 |  9 | 30 | 128 | 383  | 658
5 | 23 | 32 |  89 | 389 | 3168

**3.4 Linear Support Vector Classification using Tfidf features:**

0  |  1  |  2  |  3  |  4  |   5
--- | --- |--- | --- | --- | ---
1 | 62 | 35 |  24 |  23  |  70
2 | 28 | 60 |  50 |  58  | 112
3 | 19 | 42 | 129 | 168  | 211
4 | 12 | 18 |  93 | 314  | 771
5 |  7 | 14 |  61 | 257 | 3362

**3.5 SGD Classifier using Tfidf features:**

0  |  1  |  2  |  3  |  4  |   5
--- | --- |--- | --- | --- | ---
1 | 54 | 22 | 10 |  10  | 118
2 | 20 | 36 | 34 |  41  | 177
3 | 12 | 28 | 69 | 118  | 342
4 |  9 | 14 | 35 | 164  | 986
5 |  3 |  7 | 16 |  68 | 3607

**3.6 Random Forest Classifier using Averaged word vector features:**

0  |  1  |  2  |  3  |  4  |   5
--- | --- |--- | --- | --- | ---
1 | 21 | 24 | 34 |  48  |  87
2 | 22 | 23 | 58 |  80  | 125
3 | 20 | 27 | 86 | 171  | 265
4 | 19 | 36 | 84 | 313  | 756
5 | 22 | 56 | 94 | 399 | 3130

From the matrixes, we can see that many reviews of rating 1-4 are predicted as rating 5. This is the main reason for models' not high accuracy. 

Next step, we will try Hyperparameter Tuning to improve the metrics of models.

### 4. Hyperparameter Tuning

**4.1 Hyperparameter**

_Hyperparameters_ are parameters whose values are set prior to the commencement of the learning process. By contrast, the values of other parameters are derived via training. Different model training algorithms require different hyperparameters, some simple algorithms require none. Given these hyperparameters, the training algorithm learns the parameters from the data.

Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. 

**4.2 Hyperparameter tuning**

Hyperparameter tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. The same kind of machine learning model could require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can best solve the machine learning problem. Usually a metric is chosen to measure the algorithm's performance on an independent data set and hyperparameters that maximize this measure are adopted. Often cross-validation is used to estimate this generalization performance.

Hyperparameter tuning is performed by searching the hyper-parameter space for the best cross validation score. A search consists of:  
* an estimator (regressor or classifier);  
* a parameter space;  
* a method for searching or sampling candidates;  
* a cross-validation scheme;   
* a score function.  

**4.3 Hyperparameter tuning approaches**

Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, GridSearchCV exhaustively considers all parameter combinations, while RandomizedSearchCV can sample a given number of candidates from a parameter space with a specified distribution. 

In this project, we use the grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. 
```python
from sklearn.model_selection import GridSearchCV

def get_optimal_parameters(classifier, param_grid, 
                    train_features, train_labels, 
                    test_features, test_labels):
    # Instantiate the GridSearchCV object
    classifier_cv = GridSearchCV(classifier, param_grid, cv=5)
    # build model    
    classifier_cv.fit(train_features, train_labels)
    # predict using model
    test_predictions = classifier_cv.predict(test_features) 
    print("Tuned Parameter: {}".format(classifier_cv.best_params_))
    print("Tuned Score: {}".format(classifier_cv.best_score_))
    print()
    # evaluate model prediction performance 
    print ('Test set performance:')
    get_metrics(true_labels=test_labels, predicted_labels=test_predictions)
    # Print the optimal parameters and best score
    return classifier_cv
```

**4.4 Hyperparameter tuning result**



```python

```























